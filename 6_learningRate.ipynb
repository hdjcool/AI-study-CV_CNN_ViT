{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537a6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b12593c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 18s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "119cc945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 19:46:27.823085: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-08-10 19:46:27.823401: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467096/553467096 [==============================] - 10s 0us/step\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8809cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = tf.keras.Input((32,32,3))\n",
    "vgg = tf.keras.applications.VGG16(include_top=False)\n",
    "vgg.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65345eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vgg(input_)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(10)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67deb24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(input_, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27c0aba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, None, None, 512)   14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,719,818\n",
      "Trainable params: 5,130\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e74cb57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c837608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmming(epoch):\n",
    "    if epoch < 3:\n",
    "        return 1e-4 * (10 ** epoch)\n",
    "    elif 3 <= epoch < 75:\n",
    "        return 1e-2\n",
    "    elif 75 <= epoch < 105:\n",
    "        return 1e-3\n",
    "    else:\n",
    "        return 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f89f951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.keras.callbacks.LearningRateScheduler(warmming, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.kerasasasasasasasasas.callbacks.Callback\n",
    "tf.keras.callbacks.History\n",
    "tf.keras.callbacks.ModelCheckpoint\n",
    "tf.keras.callbacks.CSVLogger\n",
    "tf.keras.callbacks.EarlyStopping\n",
    "tf.keras.callbacks.\n",
    "tf.keras.callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3dab570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 20:01:15.520970: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-08-10 20:01:15.788855: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 23s 14ms/step - loss: 12.9351 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 4.1758 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 3/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 14.0787 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 4/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 14.7869 - lr: 0.0100\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 5/150\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 14.9075 - lr: 0.0100\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 6/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.0243 - lr: 0.0100\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 7/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 14.9703 - lr: 0.0100\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 8/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.0370 - lr: 0.0100\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 9/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1062 - lr: 0.0100\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 10/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.0771 - lr: 0.0100\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 11/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.0475 - lr: 0.0100\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 12/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.0721 - lr: 0.0100\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 13/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1058 - lr: 0.0100\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 14/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1809 - lr: 0.0100\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 15/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1630 - lr: 0.0100\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 16/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1073 - lr: 0.0100\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 17/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1596 - lr: 0.0100\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 18/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1236 - lr: 0.0100\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 19/150\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 15.1448 - lr: 0.0100\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 20/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 15.1088 - lr: 0.0100\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 21/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1829 - lr: 0.0100\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 22/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1789 - lr: 0.0100\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 23/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1622 - lr: 0.0100\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 24/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1543 - lr: 0.0100\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 25/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1062 - lr: 0.0100\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 26/150\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 15.1052 - lr: 0.0100\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 27/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 15.1688 - lr: 0.0100\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 28/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 15.1637 - lr: 0.0100\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 29/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1842 - lr: 0.0100\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 30/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1654 - lr: 0.0100\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 31/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.0840 - lr: 0.0100\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 32/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.0900 - lr: 0.0100\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 33/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1155 - lr: 0.0100\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 34/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1225 - lr: 0.0100\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 35/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.2618 - lr: 0.0100\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 36/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1806 - lr: 0.0100\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 37/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.0317 - lr: 0.0100\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 38/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1113 - lr: 0.0100\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 39/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1825 - lr: 0.0100\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 40/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1674 - lr: 0.0100\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 41/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1657 - lr: 0.0100\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 42/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1593 - lr: 0.0100\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 43/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1212 - lr: 0.0100\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 44/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1882 - lr: 0.0100\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 45/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.0913 - lr: 0.0100\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 46/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1306 - lr: 0.0100\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 47/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1051 - lr: 0.0100\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 48/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.0637 - lr: 0.0100\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 49/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.2554 - lr: 0.0100\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 50/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.2162 - lr: 0.0100\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.0964 - lr: 0.0100\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 52/150\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 15.1754 - lr: 0.0100\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 53/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 15.1291 - lr: 0.0100\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 54/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 15.1994 - lr: 0.0100\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 55/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 15.1582 - lr: 0.0100\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 56/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 15.1096 - lr: 0.0100\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 57/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 15.1955 - lr: 0.0100\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 58/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.0447 - lr: 0.0100\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 59/150\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 15.1151 - lr: 0.0100\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 60/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.2108 - lr: 0.0100\n",
      "\n",
      "Epoch 61: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 61/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.0446 - lr: 0.0100\n",
      "\n",
      "Epoch 62: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 62/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.2137 - lr: 0.0100\n",
      "\n",
      "Epoch 63: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 63/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 15.2124 - lr: 0.0100\n",
      "\n",
      "Epoch 64: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 64/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 15.1254 - lr: 0.0100\n",
      "\n",
      "Epoch 65: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 65/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.0293 - lr: 0.0100\n",
      "\n",
      "Epoch 66: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 66/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1514 - lr: 0.0100\n",
      "\n",
      "Epoch 67: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 67/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.0970 - lr: 0.0100\n",
      "\n",
      "Epoch 68: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 68/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.0509 - lr: 0.0100\n",
      "\n",
      "Epoch 69: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 69/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1929 - lr: 0.0100\n",
      "\n",
      "Epoch 70: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 70/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1020 - lr: 0.0100\n",
      "\n",
      "Epoch 71: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 71/150\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 15.1226 - lr: 0.0100\n",
      "\n",
      "Epoch 72: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 72/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 15.2317 - lr: 0.0100\n",
      "\n",
      "Epoch 73: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 73/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1425 - lr: 0.0100\n",
      "\n",
      "Epoch 74: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 74/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1390 - lr: 0.0100\n",
      "\n",
      "Epoch 75: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 75/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 15.1348 - lr: 0.0100\n",
      "\n",
      "Epoch 76: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 76/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 8.0798 - lr: 0.0010\n",
      "\n",
      "Epoch 77: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 77/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 5.8134 - lr: 0.0010\n",
      "\n",
      "Epoch 78: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 78/150\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 4.8474 - lr: 0.0010\n",
      "\n",
      "Epoch 79: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 79/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 4.2501 - lr: 0.0010\n",
      "\n",
      "Epoch 80: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 80/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 3.8468 - lr: 0.0010\n",
      "\n",
      "Epoch 81: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 81/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 3.5456 - lr: 0.0010\n",
      "\n",
      "Epoch 82: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 82/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 3.3455 - lr: 0.0010\n",
      "\n",
      "Epoch 83: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 83/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 3.1656 - lr: 0.0010\n",
      "\n",
      "Epoch 84: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 84/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 3.0198 - lr: 0.0010\n",
      "\n",
      "Epoch 85: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 85/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.9024 - lr: 0.0010\n",
      "\n",
      "Epoch 86: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 86/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.8113 - lr: 0.0010\n",
      "\n",
      "Epoch 87: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 87/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.7385 - lr: 0.0010\n",
      "\n",
      "Epoch 88: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 88/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.6901 - lr: 0.0010\n",
      "\n",
      "Epoch 89: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 89/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.6374 - lr: 0.0010\n",
      "\n",
      "Epoch 90: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 90/150\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 2.5799 - lr: 0.0010\n",
      "\n",
      "Epoch 91: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 91/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.5611 - lr: 0.0010\n",
      "\n",
      "Epoch 92: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 92/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.5272 - lr: 0.0010\n",
      "\n",
      "Epoch 93: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 93/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 2.5075 - lr: 0.0010\n",
      "\n",
      "Epoch 94: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 94/150\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 2.4870 - lr: 0.0010\n",
      "\n",
      "Epoch 95: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 95/150\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 2.4601 - lr: 0.0010\n",
      "\n",
      "Epoch 96: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 96/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 2.4427 - lr: 0.0010\n",
      "\n",
      "Epoch 97: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 97/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.4333 - lr: 0.0010\n",
      "\n",
      "Epoch 98: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 98/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.4175 - lr: 0.0010\n",
      "\n",
      "Epoch 99: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 99/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.3997 - lr: 0.0010\n",
      "\n",
      "Epoch 100: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 100/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 2.3877 - lr: 0.0010\n",
      "\n",
      "Epoch 101: LearningRateScheduler setting learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.3895 - lr: 0.0010\n",
      "\n",
      "Epoch 102: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 102/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.3766 - lr: 0.0010\n",
      "\n",
      "Epoch 103: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 103/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.3767 - lr: 0.0010\n",
      "\n",
      "Epoch 104: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 104/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.3696 - lr: 0.0010\n",
      "\n",
      "Epoch 105: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 105/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 2.3515 - lr: 0.0010\n",
      "\n",
      "Epoch 106: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 106/150\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.8118 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 107: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 107/150\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.6908 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 108: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 108/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.6556 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 109: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 109/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.6381 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 110: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 110/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.6236 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 111: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 111/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.6153 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 112: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 112/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.6078 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 113: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 113/150\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.6025 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 114: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 114/150\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.5981 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 115: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 115/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.5926 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 116: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 116/150\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.5877 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 117: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 117/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.5848 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 118: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 118/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.5833 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 119: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 119/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.5799 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 120: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 120/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.5780 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 121: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 121/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.5730 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 122: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 122/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.5732 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 123: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 123/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.5695 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 124: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 124/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.5688 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 125: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 125/150\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.5676 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 126: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 126/150\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.5643 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 127: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 127/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.5629 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 128: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 128/150\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.5596 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 129: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 129/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.5605 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 130: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 130/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5566 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 131: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 131/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.5567 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 132: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 132/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5556 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 133: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 133/150\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.5536 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 134: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 134/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5504 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 135: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 135/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.5509 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 136: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 136/150\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.5484 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 137: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 137/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5480 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 138: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 138/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5475 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 139: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 139/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5458 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 140: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 140/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5449 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 141: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 141/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5444 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 142: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 142/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5426 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 143: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 143/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5411 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 144: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 144/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5410 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 145: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 145/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5387 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 146: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 146/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5390 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 147: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 147/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5378 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 148: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 148/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5351 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 149: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5364 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 150: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 150/150\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5352 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d43c7eb0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=150, callbacks=[lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6637aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu_mac",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
